# Fundamentals of Isabelle

Let's start by creating a file `Playground.thy`
with the following contents

```
theory Playground
imports Main
begin
(* here we will write our code*)
end
```
We can use `(* *)` as comments. Those are ignored by Isabelle.

## Inductive data types

The set of natural numbers `nat` is defined inductively as

```
datatype nat = Zero | Suc nat
```

(This is not actual definition of `nat`)

We can represent `0` as `Zero`, number `1` as `Suc Zero`, number `2` as `Suc (Suc Zero)` and so on. This is widely known as [Peano arithmetic](https://en.wikipedia.org/wiki/Peano_axioms).  

## Functions

Functions can be defined usig `primrec `. For example addition can be introduced as the follows

```
primrec plus :: "nat ⇒ nat ⇒ nat" where
"plus Zero y = y" |
"plus (Suc x) y = Suc (plus x y)"
```
Isabelle uses `⇒` (written as `=>` in ASCII) to denote the type of functions. If `x` and `y` are types then `x ⇒ y` is a function type (`⇒` has [left associativity](https://en.wikipedia.org/wiki/Operator_associativity)). Such mathematical expressions must be enclosed in double quotes `" "`. After the type comes `where` keyword which marks the beginning of function **specification** (this is not function *definition*! You'll see later). Our function has two cases separated by `|`.
 The first case applies when the first argument is `Zero`. You can test it with the `value` command, for example `0+2` is

```
value "plus Zero (Suc (Suc Zero))"
```
![evaluation output](imgs/output.png)

The second case applies when the first argument is a successor `Suc x` of some number `x`. For example `1+2` outputs `3` as follows

```
value "plus (Suc Zero) (Suc (Suc Zero))" (* this prints "nat.Suc (nat.Suc (nat.Suc Zero))" *)
```


The functions `Zero` and `Suc` are called constructors. Their types are `nat`and `nat ⇒ nat` which you can check using `value`

```
value "Suc" (* this prints "nat ⇒ nat" *)
value "Zero" (* and this prints "nat" *)
```
All inductive types (`datatype`) have some size. The `size` tells us how deep the recursion is. For example

```
value "size Zero"  (* prints "0" *)
value "size (Suc Zero)" (* prints "1" *)
value "size (Suc (Suc Zero))" (* prints "2" *)
```
A constructor can take more than one argument   

```

datatype x = X1 | X2 x | X3 x x

value "size X1" (* prints "0" *)
value "size (X2 X1)" (* prints "1" *)
value "size (X3 X1 X1)" (* still prints "1" *)
value "size (X3 X1 (X2 X1))" (* now prints "2" *)

```
The value of `size` is always a finite number, which brings us to 
primitive recursion. 

## Primitive recursion

In Isabelle all functions must terminate. For example the following is illegal
```
primrec f :: "nat ⇒ nat" where
"f x = f x"
```
because the evaluation of `f` would never end. The reason why we don't want that is because it would allow us to prove any statement about `f` to be true (proofs by induction will be shown soon). 

To avoid such problems `primrec` must be a [primitive recursive function](https://en.wikipedia.org/wiki/Primitive_recursive_function). Those functions must be of the form

```
datatype x = X1 a | X2 b | ... Xn c
primrec f :: "x ⇒ y" where
"f (X1 a) = ... f a ..." |
"f (X2 b) = ... f b ..." |
... |
"f (Xn c) = ... f c ..." 
```
which means that they consider all cases (`X1 a`, `X1 b`, ... `Xn c`) of some inductive type `x` and can only perform recursion (e.g. `f a`) on constructor parameters but not on the original input (e.g. `f (X1 a)` not allowed on the right side). This means that the `size` of first argument always decreases and because it is finite and non-negative it must eventually reach `0` and end. 

Primitive recursive functions are very limiting and not [Turing-complete](https://en.wikipedia.org/wiki/Turing_completeness), therefore, if `primrec` is not enough, we can use `fun` but we may have to provide our own proof that the recursion  always ends (`fun` will be shown later).

## Theorems and proofs

To write proofs we can use `theorem` keyword. For example let's write proof that addition is associative, that is, `x+(y+z)=(x+y)+z` for all natural numers `x,y,z`

```
theorem add_associativity : "plus x (plus y z) = plus (plus x y) z" 
```
If you place your cursor below the theorem and toggle the "Proof state" button you will see the state of your proof

![proof state](imgs/proof_state.png)

When dealing with inductive types a good idea is to prove theorems by induction. This is done with `apply(induct_tac x)` tactic.

```
theorem add_associativity : "plus x (plus y z) = plus (plus x y) z" 
  apply(induct_tac x)
```
Now the state of the proof becomes

```
proof (prove)
goal (2 subgoals):
 1. plus Zero (plus y z) = plus (plus Zero y) z
 2. ⋀x. plus x (plus y z) = plus (plus x y) z ⟹
         plus (Suc x) (plus y z) = plus (plus (Suc x) y) z
```
We have two subgoals. First one is to prove that initial condition for `Zero`, second is to prove recursive case while having access to  inductive hypothesis `plus x (plus y z) = plus (plus x y) z`. The long double arrow `⟹` (written `==>` in ASCII) is the logical implication.
Symbol `⋀` is the universal quantifier "for all x". The goals we have now could be simplified. Isabelle can do a lot of the leg work automatically by using `apply(auto)` tactic.

```
theorem add_associativity : "plus x (plus y z) = plus (plus x y) z" 
  apply(induct_tac x)
   apply(auto)
  done
```
Now all the goals are proved

```
proof (prove)
goal:
No subgoals!
```
and we can add `done` to finish the theorem. 

### Querying and using proofs {#query-proofs}

If you ever forget what the theorem states, you can use command `thm` to look it up

```
thm add_associativity
```

which prints

```
plus ?x (plus ?y ?z) = plus (plus ?x ?y) ?z
```

Isabelle has added `?` to all variables, which means that from now on you can substitute
those variables for an arbitrary expression like this

```
theorem zero_plus_one_plus_z: "plus Zero (plus (Suc Zero)  z) = plus (plus Zero (Suc Zero)) z"
  apply(subst add_associativity)
  apply(rule refl)
  done
```

The `subst` command will look for any pattern that looks like `plus ?x (plus ?y ?z)` and substitute
it for `plus (plus ?x ?y) ?z`. In this case `?x` matches with `Zero`, `?y` matches `Suc Zero` and `?z` matches `z`.

If you are curious what `refl` does you can look it up in the same way

```
thm refl
```

which prints

```
?t = ?t
```

You can hold `Ctrl` (or `Cmd` on OSX) and click on any term to go to its definition. Click on `refl`
and you will be redirected to a file from Isabelle's `Main` library, to the exact line that defines `refl`

```
subsubsection ‹Axioms and basic definitions›

axiomatization where
  refl: "t = (t::'a)" and 
  subst: "s = t ⟹ P s ⟹ P t" and
  ext: "(⋀x::'a. (f x ::'b) = g x) ⟹ (λx. f x) = (λx. g x)"
    ― ‹Extensionality is built into the meta-logic, and this rule expresses
         a related property.  It is an eta-expanded version of the traditional
         rule, and similar to the ABS rule of HOL› and

  the_eq_trivial: "(THE x. x = a) = (a::'a)"
```

The proof of `zero_plus_one_plus_z` could be written more compactly as

```
theorem t: "plus Zero (plus (Suc Zero)  z) = plus (plus Zero (Suc Zero)) z"
  apply(rule add_associativity)
  done
```

Here, instead of substituting left side for the right side and then applying equality rule `refl`,
we apply the `add_associativity` directly. If both left and right sides of equation match then it is obvious that they are equal and the proof is done.

These examples show how we can use previous theorems to prove new ones. The `Main` library contains a galore of
theorems to choose from. You can search for proofs by name


![search](imgs/search1.png)
or by pattern involving wildcards `_`

![search](imgs/search2.png)

or by pattern involving `?`-prefixed variables 

![search](imgs/search3.png)


### More complex proofs {#more-complex-proofs}

Here are a few more examples of easy theorems

```
theorem add_suc_rev : " plus x (Suc y) = plus (Suc x) y" 
  by(induct_tac x, auto)
  
theorem add_suc_out : "plus (Suc x) y = Suc (plus x y)" 
  by(induct_tac x, auto)
  
theorem add_zero : "plus x Zero = x" 
  by(induct_tac x, auto)
```

The `by` keyword  is a shorthand one-line notation for simple proofs and is equivalent to the multi-line proof we did before. Let's try a more complex theorem.

```
theorem add_commutativity : "plus x y = plus y x" 
  apply(induct_tac x)
  	apply(simp)
```
We start with induction and simplification but it only gets us so far

```
proof (prove)
goal (2 subgoals):
 1. y = plus y Zero
 2. ⋀x. plus x y = plus y x ⟹ Suc (plus y x) = plus y (Suc x)
```
We have already proved the first subgoal in form of `add_zero` theorem. We can tell Isabelle to use it by using `simp add: add_zero`.

```
theorem add_commutativity : "plus x y = plus y x" 
  apply(induct_tac x)
   apply(simp add:add_zero)
```
This solves the first subgoal

```
proof (prove)
goal (1 subgoal):
 1. ⋀x. plus x y = plus y x ⟹ plus (Suc x) y = plus y (Suc x)
```

The tactic `simp` is similar to `auto` but it only applies to a single subgoal and can be extended in many ways (like with `add:`) that `auto` cannot. `auto` uses more heuristics and can solve more problems out-of-the-box. `simp` gives more control to us. Now, to solve the remaining subgoal, not even `simp` is detailed enough. We will have to tell Isabelle which substitution rules to use exactly (`simp` and `auto` do this behind the scenes). We want to apply `apply(subst add_suc_rev)`

```
proof (prove)
goal (1 subgoal):
 1. ⋀x. plus x y = plus y x ⟹ plus (Suc x) y = plus (Suc y) x
```

and then `apply(subst add_suc_out)`

```
proof (prove)
goal (1 subgoal):
 1. ⋀x. plus x y = plus y x ⟹ Suc (plus x y) = plus (Suc y) x
```

and again `apply(subst add_suc_out)`

```
proof (prove)
goal (1 subgoal):
 1. ⋀x. plus x y = plus y x ⟹ Suc (plus x y) = Suc (plus y x)
```

then we finish off with `apply(simp)`. The full proof looks as follows

```
theorem add_commutativity : "plus x y = plus y x" 
  apply(induct_tac x)
   apply(simp add:add_zero)
  apply(subst add_suc_rev)
  apply(subst add_suc_out)
  apply(subst add_suc_out)
  apply(simp)
  done
```

### The thinking process 
You might be wondering how I arrived at this proof. In fact, I started from the end. My initial goal was to prove `add_commutativity` but I couldn't. Whenever you're stuck in Isabelle, it's always a good idea to backtrack and try to prove something simpler first. I tried proving all the smaller auxiliary theorems like `add_suc_out` first. Even those were difficult because initially my definition of `plus` looked like this

```
primrec plus :: "nat ⇒ nat ⇒ nat" where
"plus Zero y = y" |
"plus (Suc x) y = plus x (Suc y)" (* Suc applied to y! *)
```
Only after changing the definition to `Suc (plus x y)` all of the theorems became much easier to prove. This is something you will see often when working with Isabelle or any other proof assistant. There are many equivalent definitions but some are more elegant than others.

## Inductive predicates

Data types are limited to representing finite entities. For example we can use `lists` to store a finite number of even numbers

```
value "[0::Nat.nat,2,4,6]"
(* it prints
"[0, 2, 4, 6]"
  :: "Nat.nat list"  <- notice Nat.nat comes from the imported Main. It's not our own definition.
*)
```
In many programming languages we can also use hash-maps to do something like

```
# This is Python code
is_even = {
 0 : True,
 1 : False,
 2 : True,
 3 : False,
 4 : True
}
```

The problem with data types is that they cannot store infinite sets. We could turn the hash-map `is_even` into a function instead

```
datatype nat = Zero | Suc nat 

fun is_even :: "nat ⇒ bool" where 
"is_even Zero = True" |
"is_even (Suc Zero) = False" |
"is_even (Suc(Suc n)) = is_even n"
```

Now calling `is_even (Suc (Suc Zero))` is equivalent to looking up a hash-map in Python `is_even[2]` but we are no longer limited to finite sets. Notice that we had to use `fun` because `primrec` does not allow nested patterns `is_even (Suc(Suc n))` on the left. We will investigate `fun` in depth later.

If functions are like infinite hash-maps then `inductive` predicates

```
inductive Even :: "nat ⇒ bool" where
zero: "Even Zero"
| double: "Even (Suc (Suc n))" if "Even n" for n
```

are like infinite linked lists.

While `is_even` returns `True` on numbers that are even and `False` on those that aren't, the `inductive` predicate `Even` can be proved to hold for even numbers but cannot be proved to hold for anything else. By analogy a list only contains elements that belong to it whereas a hash-map must also hold odd numbers and mark them as `False`.
Note one interesting difference between finite and infinite data structures. If you try to iterate over an entire linked-list or hash-map you will eventually reach the end. By exploiting this fact, you don't need to store `False` entries in a hash-map but this cannot be done with infinite structures. For this reason mathematicians refer to finite structures as [effective](https://en.wikipedia.org/wiki/Effective_method). 

To prove that `4` is even we can proceed as follows

```
theorem "Even (Suc (Suc (Suc (Suc Zero))))"
  apply(rule double)
  apply(rule double)
  apply(rule zero)
  done
```

We can also prove that `is_even x` is true only when `Even x` holds 

```
theorem "Even m ⟹ is_even m" 
  apply(induction rule: Even.induct)
  by(simp_all)
```
The long arrow `⟹` is logical implication (written `==>` in ASCII).

The tactic `apply(induction rule: Even.induct)` says that we want to prove the theorem by induction and yields the following two subgoals. 

```
 1. is_even Zero
 2. ⋀n. Even n ⟹ is_even n ⟹ is_even (nat.Suc (nat.Suc n))
```

Those are then easy to prove using `apply(rule double)` and `apply(rule zero)` so we just let Isabelle do the rest automatically by calling `by(simp_all)`. 

### When to use inductive over recursive

If you need to prove statements about the negative cases (here those would be odd numbers) then working with recursive functions is easier.
On the other hand if you do not care about negative cases and only want to prove statements about positive ones (even numbers) then `inductive` definitions often yield simpler proofs. 

In some cases it is impossible to give recursive definitions. In particular those are problems that are [not decidable](https://en.wikipedia.org/wiki/Decidability). Then our only option is to use `inductive` definitions (which is related to [semi-decidability](https://en.wikipedia.org/wiki/Decidability_(logic)#Semidecidability)).

## Type declarations and axiomatization 

### Sets {#set-def}

We can declare a new type without providing any definition. This is done with `typedecl` keyword. Of course such types are not very useful on their own so they are typically combined with some `axiomatization`. For example the following is how Isabelle defines sets

```
typedecl 'a set

axiomatization Collect :: "('a ⇒ bool) ⇒ 'a set" ― ‹comprehension›
  and member :: "'a ⇒ 'a set ⇒ bool" ― ‹membership›
  where mem_Collect_eq [iff, code_unfold]: "member a (Collect P) = P a"
    and Collect_mem_eq [simp]: "Collect (λx. member x A) = A"
```

The apostrophe in front of `'a` is a special notation that indicates that `a` is not any specific type. Instead `'a` can be substituted for any other concrete type. It works like a wildcard or like generic types in many programming languages.

We do not know anything about `'a set` other than that it can be created with function `Collect` and that its members can be queried with `member`. We know that those two functions are "inverse" of each other which more precisely expressed by axioms `mem_Collect_eq` and `Collect_mem_eq`. We can use those two facts like we would use any other theorem. Axioms are theorems that we assume to be true without giving any proofs. We also do not need to provide definitions for `Collect` and `member`. They are like "interface" functions in some programming languages.

We introduce notation 

```
notation
  member  ("'(∈')") and
  member  ("(_/ ∈ _)" [51, 51] 50)
```

so that we can write `x ∈ X` instead of `member x X`. There is also nice syntax for set comprehension

```
syntax
  "_Coll" :: "pttrn ⇒ bool ⇒ 'a set"    ("(1{_./ _})")
translations
  "{x. P}" ⇌ "CONST Collect (λx. P)"
```

so that we can write `{x . P x}` instead of `Collect P` for some predicate `P`. We will investigate `syntax` and `notation` later. Their only purpose is to make things more readable.


### Ordinals

The previous definition of natural numbers `nat` was simplified. The one actually provided by Isabelle is more complicated. Before we can see it we have to first declare `ind` but instead of definining it, we only state its axioms

```
typedecl ind

axiomatization Zero_Rep :: ind and Suc_Rep :: "ind ⇒ ind"
  ― ‹The axiom of infinity in 2 parts:›
  where Suc_Rep_inject: "Suc_Rep x = Suc_Rep y ⟹ x = y"
    and Suc_Rep_not_Zero_Rep: "Suc_Rep x ≠ Zero_Rep"

```

Notice that `ind` behaves just like `nat` but unlike `Suc`, the `Suc_Rep_inject` might be applied infinitely. It is possible to use `ind` to express infinity `∞` but `nat` could never do that (due to `size`).

Nonetheless, most of the time we **want** to work with finite numbers. For this purpose we define predicate `Nat` that includes only finite elements of `ind`

```
inductive Nat :: "ind ⇒ bool"
  where
    Zero_RepI: "Nat Zero_Rep"
  | Suc_RepI: "Nat i ⟹ Nat (Suc_Rep i)"
```


Now the real definition of `nat` is a subset of `ind set` that consists of only finite numbers `Nat`. 

```
typedef nat = "{n. Nat n}"
  morphisms Rep_Nat Abs_Nat
  using Nat.Zero_RepI by auto
```
Infinite numbers, known as [ordinals](https://en.wikipedia.org/wiki/Ordinal_number), will be covered later. The meaning of `morphisms` will be explained soon but it's not important at this point.


### All types are inhabited {#undefined}

Another important example of an axiom is

```
axiomatization undefined :: 'a
```
Other proof assistants such as Coq, Agda, Lean, etc. are based on constructive logic and in particular homotopy type theory. In those logics it is possible for a type to be empty. It is usually known as `False` or `Void` and it used to express negation. 

Isabelle is based on higher-order-logic. It can use law of excluded middle and negation out-of-the-box. As empty types are not needed anymore, Isabelle is free to assume axioms such as `undefined`. It states that every type `'a` has some inhabitant. Note that `'` is used to indicate a type variable, which we can substitue for any concrete type like `undefined::nat`.

This axiom allows us to prove things like


```
datatype bool = True | False

lemma "undefined = True ∨ undefined = False" 
  apply(case_tac "undefined::bool") 
  apply(rule disjI1)
  apply(simp)
  apply(rule disjI2)
  apply(simp)
  done
```

The tactic `apply(case_tac "undefined::bool")` tries to consider all possible cases in which something of type `bool` could be defined. `undefined` must be either `undefined = bool.True` or `undefined = bool.False`, and in both of those cases the theorem should hold. Hence, we have two subgoals

```
 1. undefined = bool.True ⟹
    undefined = bool.True ∨ undefined = bool.False
 2. undefined = bool.False ⟹
    undefined = bool.True ∨ undefined = bool.False
```
The tactic `apply(rule disjI1)` is the first (`1`) introduction (`I`) rule for disjunction (`dist`). It says that if `P` is true then `P ∨ Q` is true. Analogically `apply(rule disjI2)` says that if `Q` is true then `P ∨ Q` is true. After applying `apply(rule disjI1)` we arrive at

```
 1. undefined = bool.True ⟹ undefined = bool.True
 2. undefined = bool.False ⟹
    undefined = bool.True ∨ undefined = bool.False
```
The equality in first subgoal is trivial so we can just use `apply(simp)` to automatically simplify it. We then proceed analogically with the second subgoal.

The `undefined` axiom has some major implications which will manifest themselves later. [Here](https://www.joachim-breitner.de/blog/732-Isabelle_functions__Always_total,_sometimes_undefined) is more on this topic as well.

*Most importantly*, you should be aware that `∀𝑥. 𝑃 𝑥 → ∃𝑥. 𝑃 𝑥` is a valid first-order logic sentence. This is known as [ontological commitment](https://plato.stanford.edu/entries/ontological-commitment/) and is also explained [here](https://math.stackexchange.com/questions/1297521/for-all-x-px-implies-there-exists-x-px). Isabelle is based on first-order logic (actually higher-order logic) and commits to this law, therefore, it is [sound](https://en.wikipedia.org/wiki/Soundness) to assume such an axiom as `undefined`. 


## Meta logic 

### Meta universal quantifier

Isabelle has a meta language. You could have seen it before in form of the `⋀` symbol (written `\And` in ASCII) which works like `∀` (written `\forall` in ASCII). The difference is that `∀` is a constructed function that we imported from Isabelle's Main library

```
definition All :: "('a ⇒ bool) ⇒ bool"  (binder "∀" 10)
  where "All P ≡ (P = (λx. True))"
```

whereas `⋀` does not have any definition and is fundamental part Isabelle proof assistant itself. For example when proving something that involves `∀` weh have to use `allI` rule 

```
theorem t: "∀ x. P x"
  apply(rule allI)
```
which produces a goal

```
1. ⋀x. P x
```
You should understand it as follows. In order to prove that `P` holds for all `∀x`, Isabelle fixes an arbitrary `⋀x`. If you can prove `P x` for an arbitrary `x` then `P` must hold for all `x`. You can use tacticts such as `induct_tac x` on variables bound by `⋀`. For example

```
theorem t: "∀ x::nat. P x"
  apply(rule allI)
  apply(induct_tac x)
```
yields 2 subgoalds

```
 1. ⋀x. P 0
 2. ⋀x n. P n ⟹ P (Suc n)
```
However, if you tried to use `induct_tac x` directly when `x` is bound to `∀` you will get an error

```
theorem t: "∀ x::nat. P x"
  apply(induct_tac x) (*Error:  No such variable in subgoal: "x" *)
```

### Meta implication

There is a similar difference between logical implication `⟶` (written as `-->`) and meta-logic implication `⟹` (`==>`). For example

```
theorem t: "P x ⟶ Q x"
  apply(rule impI)
```

yields subgoal

```
1. P x ⟹ Q x
```

and now we can use `P x` as an assumption when proving `Q x`. For instance to prove that `P x ⟶ P x` we can write

```
theorem t: "P x ⟶ P x"
  apply(rule impI)
  apply(assumption)
  done
```

### Meta equality

Lastly there is logical equality `=` and meta-equality `≡` (`==`). You can use `using [[simp_trace]]` to see a detailed trace of what `simp` is doing behind the scenes 

```
theorem t: "x = y ⟹ y = x"
  using [[simp_trace]]
  apply(simp)
```
and you can see that the automatic simplifier tries to explore different possible assignments `≡` of variables (I added my comments after `#`)

```
[1]SIMPLIFIER INVOKED ON THE FOLLOWING TERM:
x = y ⟹ y = x 
[1]Adding rewrite rule "??.unknown":
x ≡ y 
[1]Applying instance of rewrite rule "??.unknown":
x ≡ y 
[1]Rewriting:
x ≡ y 
[1]Applying instance of rewrite rule "HOL.simp_thms_6":
?x1 = ?x1 ≡ True 
[1]Rewriting:
y = y ≡ True 
[1]Applying instance of rewrite rule "HOL.implies_True_equals":
(PROP ?P ⟹ True) ≡ True 
[1]Rewriting:
(x = y ⟹ True) ≡ True
```

Meta equality is also used for term substitution and rewriting.

## Abstraction and representation {#abstraction-and-representation}

Recall that when defining `nat` [here](#ordinals) 

```
typedef nat = "{n. Nat n}"
  morphisms Rep_Nat Abs_Nat
  using Nat.Zero_RepI by auto
```

we used `typedef` and `morphisms`. This is a fundamental mechanism for introducing new types. In fact, Isabelle's internal implementation of `datatype` command uses `typedef` behind the scene. 

You could imagine that every type in Isabelle is a set. Don't confuse it with the `'a set` type but instead think of each type as a "meta-logic set". Then `typedef` allows you to take any `'a set` (in this case `{n. Nat n}`) and produce a new meta-logic set (in this case `nat`). `typedef` also gives you two new functions - one called "representation" (`Rep_Nat`) and the other called "abstraction" (`Abs_Nat`). You can use abstraction to transform any element of `'a set` into an element of the meta-set (type of `Abs_Nat` is `{n. Nat n} => nat` and type of `Rep_Nat` is `nat => {n. Nat n}`). The two functions are inverse of one another and `typedef` will also automaticaly generate several theorems that we can use. For example

```
theorem "Abs_Nat (Rep_Nat x) = x"
  apply(rule Rep_Nat_inverse)
  done
```
The axiom `Rep_Nat_inverse` is auto-generated by `typedef`. You can find others by using search tab and typing in `Rep_Nat` or `Abs_Nat`

![query](imgs/search.png)

Lastly, every `typedef` must be finished with a proof of being inhabited. In this case the goal is

```
 1. ∃x. x ∈ {n. Nat n}
```

and is easily proved by `using Nat.Zero_RepI by auto`. The `using` keyword instructs `auto` to use a specific fact. There are millions of theorems and `auto` does not consider all of them because it would take too long. We have to explicitly allow it to use `Nat.Zero_RepI`.

If you want to better understand what `using Nat.Zero_RepI by auto` does, consider this more explicit proof

```
typedef nat = "{n. Nat n}"
  morphisms Rep_Nat Abs_Nat
  (* goal: ∃x. x ∈ {n. Nat n} *)
  apply(rule exI)  
  (* goal: ?x ∈ {n. Nat n} *)
  apply(subst Set.mem_Collect_eq)
  (* goal: Nat ?x *)
  apply(rule Nat.Zero_RepI)
  done
```
which shows that `nat` is inhabited because zero (`Nat.Zero_RepI`) belongs to it.

Recall that we have to prove that all types are inhabited because of the `undefined` axiom we discussed [previously](#all-types-are-inhabited).

### Morphisms can be omitted

The keyword `morphisms` is optional. We could have written

```
typedef nat = "{n. Nat n}"
  using Nat.Zero_RepI by auto

theorem "Abs_nat (Rep_nat x) = x"
  apply(rule Rep_nat_inverse)
  done
```
If we do not provide the names of abstraction and representation functions ourselves, `typedef` will auto-generate them for us. Their default names are dictated by the naming convention like on the example.

## Syntactic sugars and Lists

Isabelle defines lists as follows.

```
(* use no_notation to avoid conflicts with list already defined in Main library *)
no_notation Nil ("[]") and Cons (infixr "#" 65) and append (infixr "@" 65)

datatype 'a list =
    Nil  ("[]")
  | Cons 'a "'a list"  (infixr "#" 65)
```

The `("[]")` introduces a pretty notation for the empty list `Nil` and `(infixr "#" 65)` is a syntactic sugar for appending to a list.
For example 

```
value " x # y # z # []" (* is of type "'a list *)
``` 

is equivalent to writing

```
value "Cons x (Cons y (Cons z Nil))"
```

The keyword `infixr` specifies associativity of the operation. Compare 

```
value " x # (y # (z # []))" (* is of type "'a list *)

value "((x # y) # z) # []" (* is of type "'a list list list *)
```

with the alternative definition that uses `infixl` instead

```
datatype 'a list =
    Nil  ("[]")
  | Cons 'a "'a list"  (infixl "#" 65)
  
value " x # y # z # []" (* is of type  "'a list list list" *)

value " x # (y # (z # []))" (* is of type "'a list" *)
``` 
Also see what happens if we now switch the order of arguments of `Cons`

```
datatype 'a list =
    Nil  ("[]")
  | Cons "'a list" 'a (infixl "#" 65)
  
value "[] # x # y # z" (* is of type  "'a list" *)

value "[] # (x # (y # z))" (* is of type "'a list list list" *)
```

From now on, let's stick to our first definition, that is `Cons 'a "'a list"  (infixr "#" 65)`.
We can make our notation even more readable by introducing a custom bracket notation

```
syntax
  "_list" :: "args => 'a list"    ("[(_)]")

translations
  "[x, xs]" == "x#[xs]"
  "[x]" == "x#[]"
```
We can now write `[x,y,z]` instead of `x # y #z # []`.
The `transaltions` will be used to automatically turn 

```
value "x # y # z # []"
```
into brackets and print the following output

```
"[x, y, z]"
  :: "'a Test.list"
```

as equivalent 
We can define list concatenation as follows.

```
primrec append :: "'a list ⇒ 'a list ⇒ 'a list" (infixr "@" 65) where
append_Nil: "[] @ ys = ys" |
append_Cons: "(x#xs) @ ys = x # xs @ ys"
```

The way `syntax` and `translations` work can be easily deduced from the example. The  `(_)` that appears in `("[(_)]")` is a wildcard that matches whatever we write between `[` and `]`. Then `translations` further work on that string and rewrite it into `#` and `[]` notation.

The infix operators can be defined for any function, not only constructors. For example 

```
primrec append :: "'a list ⇒ 'a list ⇒ 'a list" (infixr "@" 65) where
append_Nil: "[] @ ys = ys" |
append_Cons: "(x#xs) @ ys = x # xs @ ys"
```

allows us to concatenate two lists using `x @ y` instead of having to write `append x y`

```
value "[x,y] @ [z,w]"
(* prints 
"[x, y, z, w]"
  :: "'a list"
*)
```

## Type classes and semigroups {#type-classes}

### List and nat are semigroups

Very often different mathematical objects share common properties. For example compare `append` for `list` with `plus` for `nat`

```
no_notation Nil ("[]") and Cons (infixr "#" 65) and append (infixr "@" 65) and plus (infixl "+" 65) 

datatype nat = Zero ("0") | Suc nat

primrec plus :: "nat ⇒ nat ⇒ nat" (infixl "+" 65) where
"0 + y = y" |
"(Suc x) + y = Suc (x + y)"

datatype 'a list =
    Nil  ("[]")
    | Cons 'a "'a list"  (infixr "#" 65)

primrec append :: "'a list ⇒ 'a list ⇒ 'a list" (infixr "@" 65) where
append_Nil: "[] @ ys = ys" |
append_Cons: "(x#xs) @ ys = x # xs @ ys"
```

It holds for both that 

```
theorem app_assoc : "x @ (y @ z) = (x @ y) @ z"
  by (induct_tac x) auto

theorem add_assoc : "x + (y + z) = (x + y) + z"
  by (induct_tac x) auto
```

In mathematics, structures like this are called *semigroups*. There are infinitely many other entities that satisfy associativity law. Many theorems about those objects
can be proved in the exact same way, without referring to the details of their definitions. We can use type classes to save ourselves work and write general proofs that will be reused for all semigroups. A programmer might know type classes under  Let's work through an example to better understand this.

### Instantiating natural numbers

Let's erase our previous code and start fresh. We start by defining `class` of all structures that can be added together using `plus` function.

```
class plus =
  fixes plus :: "'a ⇒ 'a ⇒ 'a"  (infixl "+" 65)
```

Next we have to specify that `nat` can be added and belongs to the class `plus`. This is done using `instantiation` command.

```
instantiation nat :: plus
begin
  
primrec plus_nat :: "nat ⇒ nat ⇒ nat" where
"plus_nat 0 y = y" |
"plus_nat (Suc x) y = Suc (plus_nat x y)"

instance ..
end
```

The naming convention requires us to call the function `plus_nat` instead of just `plus`. If you place your cursor right after `begin` you will see output

```
instantiation
  nat :: plus
  plus_nat == plus_class.plus :: nat ⇒ nat ⇒ nat
```
telling you that `nat` is an subtype of type `plus`. The second line says that you have to define function named `plus_nat` which will correspond to the `fixes plus`  required by the class. Once we finished defining `plus_nat` we use `instance ..` to provide all necessary proofs. In this case our class does not need any proofs so we just type `..`. 

### Instantiating lists

We proceed analogically for `list`

```

datatype 'a list =
    Nil  ("[]")
    | Cons 'a "'a list"  (infixr "#" 65)

instantiation  list :: (type) plus
begin
  
primrec plus_list :: "'a list ⇒ 'a list ⇒ 'a list" where
"plus_list []  ys = ys" |
"plus_list (x#xs) ys = x # (plus_list xs ys)"

instance ..
end
```

Notice an important detail here. The type `list` takes a type argument `'a`. We are not allowed to write

```
instantiation  "'a list" :: plus
```
Instead the type parameter becomes a parameter of `plus`.

### Restricting type parameters

The `(type)` in the snippet above means that we can accept any type in place of `'a`.  It is possible to restrict the scope of `'a` to only those types that belong some class. For example consider definining `plus` for pairs as follows

```
(a, b) + (c, d) = (a + c, b + d)
```

This requires that `a` and `c` can be added together. Same goes for `b` and `d`. To express this in Isabelle we can use pairs (also known as tuples to programmers or Cartesian product to mathematicians)

```
value "(x, y) :: nat × nat" (* with pretty syntax *)
value "(Pair x y) :: (nat, nat) prod" (* without pretty syntax *)
```

We can define `+` for pairs `'a × 'b` but  we have to assume `'a` and `'b` are restricted to class `plus` too. This can be done with `prod :: (plus, plus)` notation as follows

```
instantiation  prod :: (plus, plus) plus
begin
  
fun plus_prod :: "'a × 'b ⇒ 'a × 'b ⇒ 'a × 'b" where
"plus_prod (a, b) (c, d) = (a + c, b + d)"

instance ..
end
```

We can now use `+` on tuples like we would on any other mathematical object

```
value "(x, y)+(z, w):: nat × nat"
(* this prints
"(x + z, y + w)"
  :: "nat × nat"
*)
```

### Type classes with axioms 

Type classes are more than just collections of functions. They can also assume certain axioms that those functions must satisfy. For example semigroups  require associativity law `a+(b+c)=(a+b)+c`. This is how Isabelle defines semigroups

```
class semigroup_add = plus +
  assumes add_assoc: "(a + b) + c = a + (b + c)"
begin
```

It is a class that is a subset of `plus` class (if types are ["meta sets"](#abstraction-and-representation), then type classes are super-sets containing types and other type classes) such that `plus` satisfies `add_assoc` axiom. The `instantiation` of `semigroup_add` will now require `instance proof` block which contains proofs of the necessary axioms and ends with `qed`.

```
instantiation nat :: semigroup_add
begin
instance proof
  fix a b c :: nat
  show "(a + b) + c = a + (b + c)"
  apply(induct_tac a)
   apply(auto)
    done
qed
end
```

If you place your cursor at the end of `begin` you will see the following proof state

```
 1. ⋀a b c. a + b + c = a + (b + c)
```

We have already proved this [before](#theorems-and-proofs) using `induct_tac`. The problem now is that you can't use `induct_tac a` because `a` is not fixed yet. We use the command `fix a b c :: nat` in order to make those variables usable in our proofs. Next we have to decide which goal we want to prove by using the `show` command. In this case there is only one class axiom to prove but in other cases there might be multiple theorems that need to be proved.  After `show` we are finally able to write the proof.

### Proofs that use class axioms

You might be wondering why we've been using the word "axiom" even though we clearly had to provide its proof. Aren't axioms suppose to be theorems that have no proofs?
That was the case for axioms stated with `axiomatization` command. Here we are working with *class axioms* instead. The reason why we call `add_assoc` an axiom is because we can use it in proofs like this

```
theorem assoc_left:
fixes x y z :: "'a::semigroup_add"
shows "x + (y + z) = (x + y) + z"
using add_assoc by (rule sym)
```

The theorem `assoc_left` shows `x + (y + z) = (x + y) + z` but it assumes (`fixes`) that `x`, `y` and `z` are of any type `'a` thet belongs to  `semigroup_add`. We can now use `add_assoc` *as if* it was an axiom. We never made any reference to the type `nat`. It might as well be a `list` or something else. We don't know and it doesn't matter for the proof. The only information about `'a` that the proof is allowed to use are its class axioms. 


## Quotient types

### Integer numbers in mathematics

(If you're well-versed in mathematics and know how integers are represented, feel free to skip straight to [next section](#integer-numbers-in-isabelle).)

Sometimes we want to treat certain set elements as if they were one and the same entity. For example so far we've been only dealing with natural numbers `0, 1, 2...`
and we defined the `+` operation. The `-` operation cannot be defined on `nat` because `2-3=-1` is not a `nat`. In order to define `2-3` we have to imagine existence of some element `x` such that `x+3=2`. It is clear that `x` is not in `nat`. We can do the same trick as mathematicians did for complex numbers. We can use pairs of natural numbers and pretend that the second one is negative. Here are some examples

```
(0, 0) stands for 0-0=0
(1, 0) stands for 1-0=1
(2, 0) stands for 2-0=2
(0, 1) stands for 0-1=-1
(0, 2) stands for 0-1=-2
```

We can define addition `+` just like we defined it for pairs [before](#restricting-type-parameters). Adding two positive numbers works as expected. 

```
1+2 stands for (1,0)+(2,0)=(1+2,0+0)=(3,0) stands for 3
```

Instead of "stands for" let us write `≅` (`\cong`). Mathematicians call this congruence. Adding negative numbers works as well.

```
-1+(-2) ≅ (0,1)+(0,2)=(0+0,1+2)=(0,3) ≅ -3
```

The trouble begins when we try to add positive and negative numbers together. It turns out that `-1` can also be written as 

```
2-3≅(2,0)+(0,3)=(2,3)
3-4≅(3,0)+(0,4)=(3,4)
... and so on
```

The type `nat × nat`contains many different elements that are all congruent to the same integer. In order to define `typedef int` we need an [abstraction function](#abstraction-and-representation) `Abs_int :: nat × nat => int` that works like this

```
Abs_int (0,1) = -1
Abs_int (1,2) = -1
Abs_int (2,3) = -1
...
```

But we already said that abstraction and representation functions are inverse of one another. So the `Rep_int :: int => nat × nat` function is in fact not a function at all because it maps  one `int` to infinitely many possible `nat × nat` pairs. This cannot be done with `typedef`! Instead we need to take advantage of equivalence classes.

### Equivalence classes {#equiv-class}

A relation `R :: 'a => 'a => bool` is called equivalence relation  if it satisfies 3 ("class") axioms

1\. symmetry: `∀x y. R x y ⟶ R y x`

```
definition symp :: "('a ⇒ 'a ⇒ bool) ⇒ bool"
  where "symp R ⟷ (∀x y. R x y ⟶ R y x)"
```

This definition states that `R` is symmetric (`symp r`) if and only if (`⟷`) when `x` is in relation with `y` then `y` is in relation with `x`. (Example: If John is friends with Pete then Pete is friends with John. Anti-example: If John owes money to Pete then it doesn't necessarily mean that Pete owes money to John)

2\. reflexivity: `∀x. R x x`

```
definition reflp :: "('a ⇒ 'a ⇒ bool) ⇒ bool"
  where "reflp R ⟷ (∀x. R x x)"
```

3\. transitivity: `∀x y. x=y`

```
definition transp :: "('a ⇒ 'a ⇒ bool) ⇒ bool"
  where "transp R ⟷ (∀x y z. R x y ⟶ R y z ⟶ R x z)"
```

If some `R` satisfies all of those properties then  `R` is an equivalence relation. This is how Isabelle defines a lemma (synonym for theorem) that states `equivp R`. (Don't pay too much attention to the proof. It relies on many other lemmas you can find in `Main`. If you copy it, it won't work for you unless you prove them too.)

```
lemma equivpI: "reflp R ⟹ symp R ⟹ transp R ⟹ equivp R"
  by (auto elim: reflpE sympE transpE simp add: equivp_def)
```

When `R :: 'a => 'a => bool` is an equivalence relation then we can perform [partial application](https://en.wikipedia.org/wiki/Partial_application) of some `x :: 'a` to `R` and obtain a predicate `R x :: 'a => bool`.  This predicate defines an equivalence class. If you collect `R x` into a set `{ y . (R x) y }` you will obtain a subset of `'a`. The characteristic property of equivalence classes is that those sets are either equal (`{ y . (R x1) y } = { y . (R x2) y }` whenever `R x1 x2`) or disjoint but they cannot intersect. Therefore, Isabelle defines `equivp R` as follows.  

```
definition equivp :: "('a ⇒ 'a ⇒ bool) ⇒ bool"
  where "equivp R ⟷ (∀x y. R x y = (R x = R y))"
```

It states that predicate `R x y` holds if and only if (`=`)  the equivalence classes are equal (`R x = R y`).


### Integer numbers in Isabelle

Recall that an integer `x :: int` is represented by pair of natural numbers `(a,b) :: nat × nat` such that `x=a-b`. It is possible to check whether two integers `x = xa - xb` and `y=ya - yb` are equal without using `-` operation. This is done as follows

```
xa - xb = x = y = ya - yb \ to both sides add + xb + yb
xa + yb = ya + xb
```

Therefore, we define equivalence relation `intrel`

```
fun intrel :: "(nat × nat) ⇒ (nat × nat) ⇒ bool"
  where "intrel (xa, xb) (ya, yb) = (xa + yb = ya + xb)"
```

Now we can finally define `int` as follows 

```
quotient_type int = "nat × nat" / "intrel"
  morphisms Rep_Integ Abs_Integ
proof (rule equivpI)
  show "reflp intrel"
    by (auto simp: reflp_def)
  show "symp intrel" 
    by (auto simp: symp_def)
  show "transp intrel"
    by (auto simp: transp_def)
qed
```

The command `quotient_type` is similar to `typedef` but this time we divide `nat × nat` into equivalence classes of relation `intrel` and each class is treated as an element of the new set `int`.  The set `int` looks something like this

```
int = { ..., -2, -1, 0, 1, 2, ... }
```
where each number is a subset of `nat × nat`

```
-1 = {..., (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), ...}
0 = {..., (0, 0), (1, 1), (2, 2), (3, 3), (4, 4), ...}
1 = {..., (1, 0), (2, 1), (3, 2), (4, 3), (5, 4), ...}
```

We cannot use any relation. We have to prove that `intrel` is indeed an equivalence relation. This is done using `proof (rule equivpI)` which requires us to prove 3 goals 

```
 1. reflp intrel
 2. symp intrel
 3. transp intrel
```

You can apply `unfold` tactic like this

```
quotient_type int = "nat × nat" / "intrel"
  morphisms Rep_Integ Abs_Integ
proof (rule equivpI)
  show "reflp intrel"
    apply(unfold reflp_def)
    by (auto simp: reflp_def)
  show "symp intrel" 
    apply(unfold symp_def)
    by (auto simp: symp_def)
  show "transp intrel"
    apply(unfold transp_def)
    by (auto simp: transp_def)
qed
```

and place cursor at the end of each `unfold` to see the full statement that needs to proved

```
1. ∀x. intrel x x
2. ∀x y. intrel x y ⟶ intrel y x
3. ∀x y z. intrel x y ⟶ intrel y z ⟶ intrel x z
```

*Important remark*: The reason why we need to prove that `intrel` is an equivalence relation is because this ensures that no representation `(a,b)` stands for two different integers. Otherwise an ambiguity would arise and nothing could be proved about `int` later on.

### Lifting 

```
lift_definition plus_int :: "int ⇒ int ⇒ int"
  is "λ(x, y) (u, v). (x + u, y + v)"
  by clarsimp
```

